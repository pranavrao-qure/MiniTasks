{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning RadBert #\n",
    "Full parameter fine tuning of RadBERT model (RadBERT-RoBERTa-4m model from the RadBERT paper). Fine tuning carried out on the multi-label muti-class classification of reports, where each report can have multiple labels (For ex, a report can havel label consolidation-right and consolidation-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time, datetime\n",
    "import codecs\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass ##\n",
    "Implementing RadBERTMultiClassMulti Label PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadBERTMultiClassMultiLabel(nn.Module):\n",
    "    \"\"\"\n",
    "    RadBERTMultiClassMultiLabel: Model expects batches of natural language sentences, will\n",
    "    classify reports with multiple label\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, checkpoint, device):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.checkpoint = checkpoint\n",
    "        self.device = device\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n",
    "        self.transformer_encoder = AutoModel.from_pretrained(self.checkpoint)\n",
    "        self.transformer_encoder_hidden_size = self.transformer_encoder.config.hidden_size\n",
    "        self.linear_classifier = nn.Linear(self.transformer_encoder_hidden_size, self.num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tokenized_inp = self.tokenizer(x, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "        encoder_out = self.transformer_encoder(**tokenized_inp)\n",
    "        logits = self.linear_classifier(encoder_out.last_hidden_state[:, 0, :])\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'UCSD-VA-health/RadBERT-RoBERTa-4m'\n",
    "labels_subset = \"normal tuberculosis opacity bronchialdilation density parenchymalopacity ett aorticenlargement mediastinalwidening mediastinalmass\\\n",
    "        copd prominentbronchovascularmarkings bronchitis markings vascularprominence interval interstitiallungdisease bluntedcp effusion cardiomegaly\\\n",
    "        consolidation subtle_normal peffusion lineandtube thickening haziness hilarprominence hilar inhomogenousopacity rotation\\\n",
    "        calcification unfoldedaorta bandlikeopacity aorticcalcification aorticknucklecalcification fibrosis suture cardiacshift degenspine nodule\\\n",
    "        pneumonia inspiration fracture pneumonitis justfibrosis lesion nonaorticcalcification tuberculosispure pleuralthickening feedingtube\".split()\n",
    "num_classes = len(labels_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radbert_multi_model = RadBERTMultiClassMultiLabel(num_classes, checkpoint, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(radbert_multi_model)\n",
    "print(list(map(lambda x : x.shape, radbert_multi_model.parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss function ##\n",
    "Custom loss function for multi class multi label classification to handle uncertain tags (tags have value 0 -> absent, 1 -> present, -100 -> uncertain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassMultiLabel(nn.Module):\n",
    "    def __init__(self, uncertain_label):\n",
    "        super(MultiClassMultiLabel, self).__init__()\n",
    "        self.uncertain_label = uncertain_label\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        certain_mask = (target != self.uncertain_label)\n",
    "        loss_func = nn.MultiLabelSoftMarginLoss(weight=certain_mask.type(torch.float))\n",
    "        return loss_func(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_multilabel_loss = MultiClassMultiLabel(-100).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing MultiClassMultiLabel loss function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_tensor = torch.Tensor([[-1.0, 2.0, 1.0, 5.0, -3.0], [4.0, -2.0, 1.0, -1.0, 2.5]]).to(device)\n",
    "target_tensor_act = torch.Tensor([[0, 1, -100, 0, 0], [1, -100, 0, 0, 1]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#certain_mask = (target_tensor_act != -100)\n",
    "#print(certain_mask.type(torch.float))\n",
    "#loss_func = nn.MultiLabelSoftMarginLoss(weight=(certain_mask).type(torch.float))\n",
    "#print(loss_func(logit_tensor, target_tensor_act))\n",
    "print(multiclass_multilabel_loss(logit_tensor, target_tensor_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log, exp\n",
    "\n",
    "def log_sigmoid(x):\n",
    "    return log(1/(1+exp(-1*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = -1 * (1.0 * log_sigmoid(1.0) + 1.0 * log_sigmoid(2.0) + 0.0 * log_sigmoid(-1.0) + 1.0 * log_sigmoid(-5.0) + 1.0 * log_sigmoid(3.0)) / 5.0\n",
    "loss2 = -1 * (1.0 * log_sigmoid(4.0) + 0.0 * log_sigmoid(2.0) + 1.0 * log_sigmoid(-1.0) + 1.0 * log_sigmoid(1.0) + 1.0 * log_sigmoid(2.5)) / 5.0\n",
    "\n",
    "print(loss1)\n",
    "print(loss2)\n",
    "print((loss1 + loss2)/2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom DataLoader and Dataset ##\n",
    "Read csv file, get the report from path and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportTagsDataset(Dataset):\n",
    "    def __init__(self, tags_csv_file, report_base_path, labels_subset=None, text_transform=None, target_transform=None):\n",
    "        self.report_base_path = report_base_path\n",
    "        self.tags_csv_file = tags_csv_file\n",
    "\n",
    "        self.tags_df = pd.read_csv(self.tags_csv_file)\n",
    "        self.column_names = list(self.tags_df.columns.values)\n",
    "        self.column_names[0] = 'filename'\n",
    "        self.tags_df.columns = self.column_names\n",
    "\n",
    "        self.labels_subset = labels_subset\n",
    "        self.text_transform = text_transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.tags_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        report_path = os.path.join(self.report_base_path, self.tags_df.iloc[index, 0].split('/')[-1] + '.txt')\n",
    "        #report_text = open(report_path).read()\n",
    "        report_text = codecs.open(report_path, 'r', encoding='utf-8', errors='ignore').read()\n",
    "        if self.labels_subset is None:\n",
    "            target_list = torch.Tensor(list(self.tags_df.iloc[index][1:]))\n",
    "        else:\n",
    "            target_list = torch.Tensor(list(self.tags_df[self.labels_subset].iloc[index]))\n",
    "        return report_text, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report_base_path = \"/models_common_e2e/cxr_data/reports/training\"\n",
    "train_reports_base_path = '/home/users/pranav.rao/MiniTasks/Radbert/data/train'\n",
    "test_reports_base_path = '/home/users/pranav.rao/MiniTasks/Radbert/data/test'\n",
    "train_tags_file = '/home/users/pranav.rao/Downloads/report_tags_25k_train.csv'\n",
    "test_tags_file = '/home/users/pranav.rao/Downloads/report_tags_25k_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ReportTagsDataset(train_tags_file, train_reports_base_path, labels_subset=labels_subset)\n",
    "test_data = ReportTagsDataset(test_tags_file, test_reports_base_path, labels_subset=labels_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "#train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Dataset and DataLoader ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features)\n",
    "print(train_labels)\n",
    "print(f\"Feature batch shape: {len(train_features)}\")\n",
    "print(f\"Labels batch shape: {len(train_labels)}\")\n",
    "report_text = train_features[0]\n",
    "print(report_text)\n",
    "label = train_labels[0]\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logit_tensor = radbert_multi_model(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logit_tensor)\n",
    "print(logit_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning ##\n",
    "Fine tuning the RadBERT model for tags prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimizer ###\n",
    "Using Adam optimizer with learning rate 3e-5, beta1 = 0.9, beta2 = 0.99, l2 weight decay of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-5\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "l2_weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(radbert_multi_model.parameters(), lr=lr, betas=(beta1, beta2), weight_decay=l2_weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        inputs, labels = data\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = radbert_multi_model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = multiclass_multilabel_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:\n",
    "            last_loss = running_loss / 50 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full fine-tuning loog ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "    radbert_multi_model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch, writer)\n",
    "    model_path = '/home/users/pranav.rao/MiniTasks/Radbert/ModelPool/model_{}_{}'.format(timestamp, epoch)\n",
    "    torch.save(radbert_multi_model.state_dict(), model_path)\n",
    "\n",
    "    # Set the model to evaluation mode, disabling dropout and using population, statistics for batch normalization\n",
    "    running_vloss = 0.0\n",
    "    radbert_multi_model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vlabels = vlabels.to(device)\n",
    "            voutputs = radbert_multi_model(vinputs)\n",
    "            vloss = multiclass_multilabel_loss(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch + 1)\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing fine-tuned models ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [\"The report shows small right-sided pleural effusion\", \"The report shows small left-sided pleural effusion\",\\\n",
    "    \"The report shows large right-sided pleural effusion\", \"The report shows large left-sided pleural effusion\",\\\n",
    "    \"There are no abnormalities in the report\",\\\n",
    "    \"There is severe consolidation in the left side\",\"There is severe consolidation in the right side\",\\\n",
    "    \"There is mild consolidation in the right side\", \"There is mild consolidation in the left side\"\n",
    "]\n",
    "\n",
    "sentence1_base = \"A <SizeModifier> <AbnormalReport> can be seen in the report in the <LocationModifier> part\"\n",
    "sentence2_base = \"The report shows a <SizeModifier> <LocationModifier> <AbnormalReport>\"\n",
    "size_modifiers = ['small', 'large']\n",
    "loc_modifiers = ['upper-left', 'lower-left', 'right-sided', 'left-sided']\n",
    "abnormal_report = ['pleural effusion']\n",
    "\n",
    "l1 = [sentence1_base.replace('<SizeModifier>', size_mod).replace('<LocationModifier>', loc_mod).replace('<AbnormalReport>', ab_rep) for size_mod, loc_mod, ab_rep in product(size_modifiers, loc_modifiers, abnormal_report)]\n",
    "l2 = [sentence2_base.replace('<SizeModifier>', size_mod).replace('<LocationModifier>', loc_mod).replace('<AbnormalReport>', ab_rep) for size_mod, loc_mod, ab_rep in product(size_modifiers, loc_modifiers, abnormal_report)]\n",
    "\n",
    "negative_sentences = ['The report shows no pleural effusion', 'The report shows no consolidation on any side']\n",
    "all_sentence_list = l1 + l2 + negative_sentences + sentence_list[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(all_sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine_sim_matrix(sentence_embeddings):\n",
    "    #stacked_sentence_embeddings = torch.stack(sentence_embeddings)\n",
    "    stacked_sentence_embeddings = sentence_embeddings\n",
    "    # Calculate the cosine similarity matrix\n",
    "    cosine_sim_matrix = F.cosine_similarity(stacked_sentence_embeddings.unsqueeze(1), stacked_sentence_embeddings.unsqueeze(0), dim=2)\n",
    "    return stacked_sentence_embeddings, cosine_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(model, input):\n",
    "    with torch.no_grad():\n",
    "        tokenized_inp = model.tokenizer(input, padding=True, truncation=True, return_tensors='pt')\n",
    "        encoder_out = model.transformer_encoder(**tokenized_inp)\n",
    "    return encoder_out.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/users/pranav.rao/MiniTasks/Radbert/ModelPool/model_20230912_143355_0'\n",
    "checkpoint = 'UCSD-VA-health/RadBERT-RoBERTa-4m'\n",
    "radbert_multi_model = RadBERTMultiClassMultiLabel(num_classes, checkpoint)\n",
    "radbert_multi_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = get_sentence_embeddings(radbert_multi_model, all_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_embeddings)\n",
    "print(all_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cosine_sim = calc_cosine_sim_matrix(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the predictions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, input):\n",
    "    with torch.no_grad():\n",
    "        tokenized_inp = model.tokenizer(input, padding=True, truncation=True, return_tensors='pt')\n",
    "        encoder_out = model.transformer_encoder(**tokenized_inp)\n",
    "        logits = model.linear_classifier(encoder_out.last_hidden_state[:, 0, :])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_predictions(radbert_multi_model, all_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)\n",
    "print(predictions.shape)\n",
    "print(predictions.argmax(dim=-1))\n",
    "print(predictions[:, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = pd.read_csv('/home/users/pranav.rao/MiniTasks/Radbert/report_tags_25k_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df_tags.columns.values)\n",
    "print(columns[224])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TutorialCuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
